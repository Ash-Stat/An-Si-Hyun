{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "분류.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPE8DIOJWIfvY46Z/xLv0Bq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ash-Stat/An-Si-Hyun/blob/data/%EB%B6%84%EB%A5%98%20%EB%B0%8F%20%EC%97%AC%EB%9F%AC%20%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC%20%EC%A0%95%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys447qqrKeIz"
      },
      "source": [
        "#MNIST 데이터셋 내려받기\r\n",
        "\r\n",
        "샘플이 하나의 행, 특성이 하나의 열로 구성된 배열을 가진 data 키\r\n",
        "\r\n",
        "레이블 배열을 담은 target키"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOkHFXjdKyUZ",
        "outputId": "fb2c8734-e79c-4a20-d238-d05811e399a8"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\r\n",
        "mnist = fetch_openml('mnist_784', version = 1)\r\n",
        "mnist.keys()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'DESCR', 'details', 'categories', 'url'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4O9apktLbXh"
      },
      "source": [
        "# 데이터 담기\r\n",
        "\r\n",
        "784는 28x28 픽셀을 이미지가 갖고있기 때문이다.\r\n",
        "\r\n",
        "이는 0(흰색)부터 255(검은색)까지의 픽셀 강도를 나타낸다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMjQvSVAL-iQ",
        "outputId": "da9c072f-c839-4074-a1c5-8e939a481fe0"
      },
      "source": [
        "x, y=mnist['data'],mnist['target']\r\n",
        "print('데이터 개수, 픽셀확인 :', x.shape)\r\n",
        "print('타겟 데이터 확인 :', y.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "데이터 개수, 픽셀확인 : (70000, 784)\n",
            "타겟 데이터 확인 : (70000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaWF9vusMFDT"
      },
      "source": [
        "# 샘플의 특성 벡터를 추출해서 28x28 배열로 크기를 바꾸고 matplotlib의 imshow() 함수를 사용해서 그리면 된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "V23JMoEhNAiM",
        "outputId": "71946842-1f82-4cb0-da27-7fb488de5522"
      },
      "source": [
        "import matplotlib as mpl\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "some_digit = x[0] # 첫번째 이미지 갖고오기\r\n",
        "\r\n",
        "some_digit_image = some_digit.reshape(28,28)\r\n",
        "\r\n",
        "plt.imshow(some_digit_image, cmap='binary') # cmap을 binary로 설정해주면, 흰색 바탕에 검은 글씨로 나타난다.\r\n",
        "plt.axis('off')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "print(y[0]) # 정답확인"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbreIrCZN_Ge"
      },
      "source": [
        "# Y값은 문자열로 되어있고, 대부분 머신러닝 알고리즘은 숫자를 이용하므로 y를 정수로 변환해야한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYB7VYudNUDD",
        "outputId": "8782cf2b-cd4f-4cd3-c8d9-bbc83bb71d3a"
      },
      "source": [
        "y"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['5', '0', '4', ..., '4', '5', '6'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlaFmZZ7N8DP"
      },
      "source": [
        "import numpy as np\r\n",
        "y=y.astype(np.uint8) # astype(float)해도 된다."
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_gTrY0WOJaP"
      },
      "source": [
        "# 훈련, 테스트 셋 나누기\r\n",
        "어떤 학습 알고리즘은 훈련 샘플의 순서에 민감해서 많은 비슷한 샘플이 연이어 나타나면 성능이 나빠진다.\r\n",
        "그래서 데이터셋을 섞으면 이런 문제를 방지할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6FLJYc-OJ7J"
      },
      "source": [
        "x_train,x_test,y_train,y_test = x[:60000], x[60000:], y[:60000], y[60000:]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la5n3LM2PFrh"
      },
      "source": [
        "# 이진 분류기 훈련\r\n",
        "동전 뒤집기처럼 yes or no로 클래스를 구분할 수 있는 이진 분류기의 한 예이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYiI9pxLPOz-"
      },
      "source": [
        "y_train_5=(y_train == 5)\r\n",
        "\r\n",
        "y_test_5 = (y_test == 5)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOXepuADPZud",
        "outputId": "205ceb0a-60d6-4c27-ccf4-c2a3d74aa45b"
      },
      "source": [
        "y_train_5"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True, False, False, ...,  True, False, False])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BnT0MyVPbB9"
      },
      "source": [
        "# Stochastic Gradient Descent(SGD) 확률적 경사 하강법\r\n",
        "\r\n",
        "한 번에 하나씩 훈련 샘플을 독립적으로 처리한다.\r\n",
        "\r\n",
        "SGDClassifier는 훈련하는 데 무작위성을 이용한다.(그래서 확률적이라 한다.)\r\n",
        "결과를 재현하고 싶다면 random_state 매개변수를 지정해야한다.\r\n",
        "\r\n",
        "사이킷런의 SDGClassifier와 SGDRegressor는 기본적으로 에포크마다 훈련데이터를 다시 섞는다.(iteration 마다)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ph1iiW9vcLLb",
        "outputId": "50f7f9b5-bc62-441a-a512-7c83ac837fab"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\r\n",
        "\r\n",
        "sgd_clf = SGDClassifier(random_state=42, verbose=1)\r\n",
        "sgd_clf.fit(x_train, y_train_5)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- Epoch 1\n",
            "Norm: 8147.16, NNZs: 624, Bias: 12.352211, T: 60000, Avg. loss: 96002.101183\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 4850.19, NNZs: 630, Bias: 21.765171, T: 120000, Avg. loss: 13448.913390\n",
            "Total training time: 0.21 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 3609.24, NNZs: 639, Bias: 26.212700, T: 180000, Avg. loss: 7640.779336\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 2958.05, NNZs: 642, Bias: 29.904666, T: 240000, Avg. loss: 5411.680114\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 2535.62, NNZs: 643, Bias: 32.474667, T: 300000, Avg. loss: 4222.549757\n",
            "Total training time: 0.53 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 2202.83, NNZs: 646, Bias: 34.517745, T: 360000, Avg. loss: 3421.870201\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 1972.29, NNZs: 647, Bias: 36.129522, T: 420000, Avg. loss: 2905.781392\n",
            "Total training time: 0.74 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 1801.26, NNZs: 649, Bias: 37.668583, T: 480000, Avg. loss: 2539.335154\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 1672.62, NNZs: 650, Bias: 38.899898, T: 540000, Avg. loss: 2191.478449\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 1548.22, NNZs: 652, Bias: 40.261223, T: 600000, Avg. loss: 1990.707804\n",
            "Total training time: 1.05 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 1438.91, NNZs: 652, Bias: 41.578296, T: 660000, Avg. loss: 1781.978793\n",
            "Total training time: 1.16 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 1352.18, NNZs: 653, Bias: 42.664908, T: 720000, Avg. loss: 1620.050574\n",
            "Total training time: 1.26 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 1275.69, NNZs: 657, Bias: 43.632250, T: 780000, Avg. loss: 1487.411631\n",
            "Total training time: 1.37 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 1202.94, NNZs: 660, Bias: 44.355033, T: 840000, Avg. loss: 1362.608392\n",
            "Total training time: 1.47 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 1156.67, NNZs: 661, Bias: 45.337357, T: 900000, Avg. loss: 1298.163486\n",
            "Total training time: 1.57 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 1092.29, NNZs: 664, Bias: 46.164884, T: 960000, Avg. loss: 1196.751623\n",
            "Total training time: 1.68 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 1051.19, NNZs: 664, Bias: 46.862769, T: 1020000, Avg. loss: 1149.201647\n",
            "Total training time: 1.79 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 1005.43, NNZs: 666, Bias: 47.588147, T: 1080000, Avg. loss: 1047.008309\n",
            "Total training time: 1.89 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 968.87, NNZs: 666, Bias: 48.252823, T: 1140000, Avg. loss: 1011.144365\n",
            "Total training time: 2.00 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 932.43, NNZs: 667, Bias: 48.948382, T: 1200000, Avg. loss: 963.293122\n",
            "Total training time: 2.10 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 895.20, NNZs: 668, Bias: 49.467357, T: 1260000, Avg. loss: 895.387752\n",
            "Total training time: 2.21 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 867.67, NNZs: 668, Bias: 49.930425, T: 1320000, Avg. loss: 850.320633\n",
            "Total training time: 2.31 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 841.29, NNZs: 668, Bias: 50.514509, T: 1380000, Avg. loss: 839.394474\n",
            "Total training time: 2.42 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 814.53, NNZs: 668, Bias: 51.112224, T: 1440000, Avg. loss: 790.698319\n",
            "Total training time: 2.52 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 789.80, NNZs: 668, Bias: 51.615752, T: 1500000, Avg. loss: 757.911519\n",
            "Total training time: 2.63 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 763.85, NNZs: 668, Bias: 52.072735, T: 1560000, Avg. loss: 737.683136\n",
            "Total training time: 2.73 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 738.11, NNZs: 668, Bias: 52.599855, T: 1620000, Avg. loss: 698.935835\n",
            "Total training time: 2.83 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 719.01, NNZs: 668, Bias: 53.114249, T: 1680000, Avg. loss: 664.873345\n",
            "Total training time: 2.94 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 705.82, NNZs: 668, Bias: 53.576601, T: 1740000, Avg. loss: 649.748778\n",
            "Total training time: 3.05 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 688.66, NNZs: 668, Bias: 53.920562, T: 1800000, Avg. loss: 628.627618\n",
            "Total training time: 3.15 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 673.35, NNZs: 668, Bias: 54.432767, T: 1860000, Avg. loss: 619.005568\n",
            "Total training time: 3.26 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 653.53, NNZs: 668, Bias: 54.814255, T: 1920000, Avg. loss: 574.672141\n",
            "Total training time: 3.36 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 638.38, NNZs: 668, Bias: 55.106061, T: 1980000, Avg. loss: 567.035768\n",
            "Total training time: 3.46 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 624.77, NNZs: 668, Bias: 55.513956, T: 2040000, Avg. loss: 557.909286\n",
            "Total training time: 3.57 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 607.74, NNZs: 668, Bias: 55.875809, T: 2100000, Avg. loss: 541.177783\n",
            "Total training time: 3.67 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 597.01, NNZs: 668, Bias: 56.292974, T: 2160000, Avg. loss: 537.958671\n",
            "Total training time: 3.77 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 586.01, NNZs: 668, Bias: 56.581190, T: 2220000, Avg. loss: 507.351183\n",
            "Total training time: 3.88 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 573.84, NNZs: 668, Bias: 56.891543, T: 2280000, Avg. loss: 493.888910\n",
            "Total training time: 3.98 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 563.55, NNZs: 668, Bias: 57.203349, T: 2340000, Avg. loss: 485.760041\n",
            "Total training time: 4.09 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 553.79, NNZs: 668, Bias: 57.536161, T: 2400000, Avg. loss: 470.996814\n",
            "Total training time: 4.19 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 539.04, NNZs: 668, Bias: 57.815736, T: 2460000, Avg. loss: 461.221635\n",
            "Total training time: 4.29 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 532.07, NNZs: 669, Bias: 58.072873, T: 2520000, Avg. loss: 440.303797\n",
            "Total training time: 4.40 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 525.62, NNZs: 669, Bias: 58.394532, T: 2580000, Avg. loss: 434.999480\n",
            "Total training time: 4.50 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 514.37, NNZs: 669, Bias: 58.708471, T: 2640000, Avg. loss: 430.816509\n",
            "Total training time: 4.61 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 507.22, NNZs: 669, Bias: 59.000199, T: 2700000, Avg. loss: 420.158719\n",
            "Total training time: 4.71 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 497.27, NNZs: 669, Bias: 59.263927, T: 2760000, Avg. loss: 408.582692\n",
            "Total training time: 4.81 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 489.02, NNZs: 669, Bias: 59.507545, T: 2820000, Avg. loss: 395.121284\n",
            "Total training time: 4.92 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 482.55, NNZs: 669, Bias: 59.742729, T: 2880000, Avg. loss: 385.115573\n",
            "Total training time: 5.02 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 474.85, NNZs: 669, Bias: 60.041635, T: 2940000, Avg. loss: 381.367595\n",
            "Total training time: 5.12 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 468.78, NNZs: 669, Bias: 60.313984, T: 3000000, Avg. loss: 375.000796\n",
            "Total training time: 5.23 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 460.73, NNZs: 669, Bias: 60.587382, T: 3060000, Avg. loss: 370.139893\n",
            "Total training time: 5.33 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 455.13, NNZs: 669, Bias: 60.842921, T: 3120000, Avg. loss: 358.820543\n",
            "Total training time: 5.44 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 445.99, NNZs: 669, Bias: 61.128640, T: 3180000, Avg. loss: 350.158900\n",
            "Total training time: 5.54 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 441.41, NNZs: 669, Bias: 61.330785, T: 3240000, Avg. loss: 345.638348\n",
            "Total training time: 5.64 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 434.72, NNZs: 669, Bias: 61.532431, T: 3300000, Avg. loss: 339.961481\n",
            "Total training time: 5.75 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 426.20, NNZs: 669, Bias: 61.754551, T: 3360000, Avg. loss: 333.168536\n",
            "Total training time: 5.85 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 424.22, NNZs: 669, Bias: 61.978713, T: 3420000, Avg. loss: 321.508298\n",
            "Total training time: 5.95 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 417.37, NNZs: 669, Bias: 62.219313, T: 3480000, Avg. loss: 319.925421\n",
            "Total training time: 6.06 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 412.58, NNZs: 669, Bias: 62.413198, T: 3540000, Avg. loss: 315.700786\n",
            "Total training time: 6.17 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 405.43, NNZs: 669, Bias: 62.651299, T: 3600000, Avg. loss: 311.330719\n",
            "Total training time: 6.27 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 401.92, NNZs: 669, Bias: 62.857796, T: 3660000, Avg. loss: 306.183402\n",
            "Total training time: 6.38 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 398.50, NNZs: 669, Bias: 63.061007, T: 3720000, Avg. loss: 296.562722\n",
            "Total training time: 6.48 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 393.65, NNZs: 670, Bias: 63.244955, T: 3780000, Avg. loss: 299.581658\n",
            "Total training time: 6.58 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 390.55, NNZs: 670, Bias: 63.457773, T: 3840000, Avg. loss: 288.295780\n",
            "Total training time: 6.69 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 385.33, NNZs: 670, Bias: 63.651414, T: 3900000, Avg. loss: 284.072291\n",
            "Total training time: 6.80 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 379.70, NNZs: 670, Bias: 63.821937, T: 3960000, Avg. loss: 283.933892\n",
            "Total training time: 6.90 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 375.73, NNZs: 670, Bias: 64.012416, T: 4020000, Avg. loss: 281.800068\n",
            "Total training time: 7.00 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 372.68, NNZs: 670, Bias: 64.160414, T: 4080000, Avg. loss: 273.498049\n",
            "Total training time: 7.11 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 368.93, NNZs: 670, Bias: 64.354665, T: 4140000, Avg. loss: 273.055591\n",
            "Total training time: 7.21 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 363.20, NNZs: 670, Bias: 64.524832, T: 4200000, Avg. loss: 268.383538\n",
            "Total training time: 7.32 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 360.68, NNZs: 670, Bias: 64.713734, T: 4260000, Avg. loss: 255.128462\n",
            "Total training time: 7.42 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 355.56, NNZs: 670, Bias: 64.890710, T: 4320000, Avg. loss: 259.772483\n",
            "Total training time: 7.52 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 353.22, NNZs: 670, Bias: 65.104483, T: 4380000, Avg. loss: 248.792869\n",
            "Total training time: 7.63 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 349.07, NNZs: 670, Bias: 65.276797, T: 4440000, Avg. loss: 247.528945\n",
            "Total training time: 7.73 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 345.28, NNZs: 670, Bias: 65.428892, T: 4500000, Avg. loss: 249.966328\n",
            "Total training time: 7.83 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 342.09, NNZs: 670, Bias: 65.625324, T: 4560000, Avg. loss: 240.859066\n",
            "Total training time: 7.94 seconds.\n",
            "-- Epoch 77\n",
            "Norm: 337.95, NNZs: 670, Bias: 65.795231, T: 4620000, Avg. loss: 241.887451\n",
            "Total training time: 8.04 seconds.\n",
            "-- Epoch 78\n",
            "Norm: 335.35, NNZs: 670, Bias: 65.952007, T: 4680000, Avg. loss: 244.884590\n",
            "Total training time: 8.14 seconds.\n",
            "-- Epoch 79\n",
            "Norm: 333.70, NNZs: 670, Bias: 66.106893, T: 4740000, Avg. loss: 233.932490\n",
            "Total training time: 8.24 seconds.\n",
            "-- Epoch 80\n",
            "Norm: 330.06, NNZs: 670, Bias: 66.285029, T: 4800000, Avg. loss: 235.706971\n",
            "Total training time: 8.34 seconds.\n",
            "-- Epoch 81\n",
            "Norm: 326.91, NNZs: 670, Bias: 66.438381, T: 4860000, Avg. loss: 226.755685\n",
            "Total training time: 8.45 seconds.\n",
            "-- Epoch 82\n",
            "Norm: 324.40, NNZs: 670, Bias: 66.579333, T: 4920000, Avg. loss: 226.359413\n",
            "Total training time: 8.55 seconds.\n",
            "-- Epoch 83\n",
            "Norm: 320.33, NNZs: 670, Bias: 66.710641, T: 4980000, Avg. loss: 228.050199\n",
            "Total training time: 8.65 seconds.\n",
            "-- Epoch 84\n",
            "Norm: 318.15, NNZs: 670, Bias: 66.862188, T: 5040000, Avg. loss: 218.693318\n",
            "Total training time: 8.76 seconds.\n",
            "-- Epoch 85\n",
            "Norm: 315.74, NNZs: 670, Bias: 66.994248, T: 5100000, Avg. loss: 215.561642\n",
            "Total training time: 8.86 seconds.\n",
            "-- Epoch 86\n",
            "Norm: 313.32, NNZs: 670, Bias: 67.132370, T: 5160000, Avg. loss: 217.206156\n",
            "Total training time: 8.97 seconds.\n",
            "-- Epoch 87\n",
            "Norm: 309.38, NNZs: 670, Bias: 67.257639, T: 5220000, Avg. loss: 215.176613\n",
            "Total training time: 9.07 seconds.\n",
            "-- Epoch 88\n",
            "Norm: 307.08, NNZs: 670, Bias: 67.398617, T: 5280000, Avg. loss: 212.157832\n",
            "Total training time: 9.18 seconds.\n",
            "-- Epoch 89\n",
            "Norm: 305.86, NNZs: 670, Bias: 67.535998, T: 5340000, Avg. loss: 206.156508\n",
            "Total training time: 9.28 seconds.\n",
            "-- Epoch 90\n",
            "Norm: 302.76, NNZs: 670, Bias: 67.657030, T: 5400000, Avg. loss: 206.878159\n",
            "Total training time: 9.38 seconds.\n",
            "-- Epoch 91\n",
            "Norm: 301.82, NNZs: 670, Bias: 67.798744, T: 5460000, Avg. loss: 197.264274\n",
            "Total training time: 9.49 seconds.\n",
            "-- Epoch 92\n",
            "Norm: 298.16, NNZs: 670, Bias: 67.933567, T: 5520000, Avg. loss: 205.209018\n",
            "Total training time: 9.60 seconds.\n",
            "-- Epoch 93\n",
            "Norm: 296.22, NNZs: 670, Bias: 68.084916, T: 5580000, Avg. loss: 198.246363\n",
            "Total training time: 9.70 seconds.\n",
            "-- Epoch 94\n",
            "Norm: 293.79, NNZs: 670, Bias: 68.220313, T: 5640000, Avg. loss: 200.283868\n",
            "Total training time: 9.80 seconds.\n",
            "-- Epoch 95\n",
            "Norm: 291.58, NNZs: 672, Bias: 68.315390, T: 5700000, Avg. loss: 198.349566\n",
            "Total training time: 9.91 seconds.\n",
            "-- Epoch 96\n",
            "Norm: 288.86, NNZs: 672, Bias: 68.441096, T: 5760000, Avg. loss: 191.799926\n",
            "Total training time: 10.01 seconds.\n",
            "-- Epoch 97\n",
            "Norm: 286.52, NNZs: 672, Bias: 68.551650, T: 5820000, Avg. loss: 188.067543\n",
            "Total training time: 10.11 seconds.\n",
            "-- Epoch 98\n",
            "Norm: 283.18, NNZs: 672, Bias: 68.674702, T: 5880000, Avg. loss: 188.925565\n",
            "Total training time: 10.21 seconds.\n",
            "-- Epoch 99\n",
            "Norm: 282.82, NNZs: 672, Bias: 68.793230, T: 5940000, Avg. loss: 184.451618\n",
            "Total training time: 10.32 seconds.\n",
            "-- Epoch 100\n",
            "Norm: 280.59, NNZs: 672, Bias: 68.910563, T: 6000000, Avg. loss: 181.729408\n",
            "Total training time: 10.42 seconds.\n",
            "-- Epoch 101\n",
            "Norm: 277.06, NNZs: 672, Bias: 69.041497, T: 6060000, Avg. loss: 183.654123\n",
            "Total training time: 10.52 seconds.\n",
            "-- Epoch 102\n",
            "Norm: 275.04, NNZs: 672, Bias: 69.156406, T: 6120000, Avg. loss: 182.615407\n",
            "Total training time: 10.63 seconds.\n",
            "-- Epoch 103\n",
            "Norm: 273.11, NNZs: 673, Bias: 69.274971, T: 6180000, Avg. loss: 177.496512\n",
            "Total training time: 10.74 seconds.\n",
            "-- Epoch 104\n",
            "Norm: 272.20, NNZs: 673, Bias: 69.403735, T: 6240000, Avg. loss: 180.174390\n",
            "Total training time: 10.84 seconds.\n",
            "-- Epoch 105\n",
            "Norm: 269.19, NNZs: 673, Bias: 69.505723, T: 6300000, Avg. loss: 172.939035\n",
            "Total training time: 10.95 seconds.\n",
            "-- Epoch 106\n",
            "Norm: 266.32, NNZs: 673, Bias: 69.628901, T: 6360000, Avg. loss: 173.367213\n",
            "Total training time: 11.05 seconds.\n",
            "-- Epoch 107\n",
            "Norm: 265.12, NNZs: 673, Bias: 69.766549, T: 6420000, Avg. loss: 172.235022\n",
            "Total training time: 11.15 seconds.\n",
            "-- Epoch 108\n",
            "Norm: 263.56, NNZs: 673, Bias: 69.892083, T: 6480000, Avg. loss: 172.565065\n",
            "Total training time: 11.26 seconds.\n",
            "-- Epoch 109\n",
            "Norm: 262.23, NNZs: 673, Bias: 69.982738, T: 6540000, Avg. loss: 170.529225\n",
            "Total training time: 11.36 seconds.\n",
            "-- Epoch 110\n",
            "Norm: 260.10, NNZs: 673, Bias: 70.104479, T: 6600000, Avg. loss: 170.021078\n",
            "Total training time: 11.47 seconds.\n",
            "-- Epoch 111\n",
            "Norm: 257.44, NNZs: 673, Bias: 70.219045, T: 6660000, Avg. loss: 170.345847\n",
            "Total training time: 11.57 seconds.\n",
            "-- Epoch 112\n",
            "Norm: 255.68, NNZs: 673, Bias: 70.319220, T: 6720000, Avg. loss: 166.655952\n",
            "Total training time: 11.67 seconds.\n",
            "-- Epoch 113\n",
            "Norm: 253.43, NNZs: 674, Bias: 70.427429, T: 6780000, Avg. loss: 166.020863\n",
            "Total training time: 11.77 seconds.\n",
            "-- Epoch 114\n",
            "Norm: 251.89, NNZs: 674, Bias: 70.553693, T: 6840000, Avg. loss: 165.323846\n",
            "Total training time: 11.88 seconds.\n",
            "-- Epoch 115\n",
            "Norm: 250.24, NNZs: 674, Bias: 70.670167, T: 6900000, Avg. loss: 161.073211\n",
            "Total training time: 11.98 seconds.\n",
            "-- Epoch 116\n",
            "Norm: 249.31, NNZs: 674, Bias: 70.779819, T: 6960000, Avg. loss: 157.121875\n",
            "Total training time: 12.08 seconds.\n",
            "-- Epoch 117\n",
            "Norm: 247.75, NNZs: 674, Bias: 70.878567, T: 7020000, Avg. loss: 156.244618\n",
            "Total training time: 12.19 seconds.\n",
            "-- Epoch 118\n",
            "Norm: 245.94, NNZs: 674, Bias: 70.984939, T: 7080000, Avg. loss: 154.084674\n",
            "Total training time: 12.29 seconds.\n",
            "-- Epoch 119\n",
            "Norm: 244.14, NNZs: 674, Bias: 71.096117, T: 7140000, Avg. loss: 151.507286\n",
            "Total training time: 12.39 seconds.\n",
            "-- Epoch 120\n",
            "Norm: 243.32, NNZs: 674, Bias: 71.211897, T: 7200000, Avg. loss: 155.444041\n",
            "Total training time: 12.50 seconds.\n",
            "-- Epoch 121\n",
            "Norm: 242.31, NNZs: 674, Bias: 71.318420, T: 7260000, Avg. loss: 151.539514\n",
            "Total training time: 12.61 seconds.\n",
            "-- Epoch 122\n",
            "Norm: 241.18, NNZs: 674, Bias: 71.404827, T: 7320000, Avg. loss: 154.754492\n",
            "Total training time: 12.71 seconds.\n",
            "-- Epoch 123\n",
            "Norm: 240.13, NNZs: 674, Bias: 71.478196, T: 7380000, Avg. loss: 149.354095\n",
            "Total training time: 12.81 seconds.\n",
            "-- Epoch 124\n",
            "Norm: 239.49, NNZs: 674, Bias: 71.564566, T: 7440000, Avg. loss: 147.239049\n",
            "Total training time: 12.92 seconds.\n",
            "-- Epoch 125\n",
            "Norm: 238.85, NNZs: 674, Bias: 71.648900, T: 7500000, Avg. loss: 145.454548\n",
            "Total training time: 13.02 seconds.\n",
            "-- Epoch 126\n",
            "Norm: 237.27, NNZs: 674, Bias: 71.739182, T: 7560000, Avg. loss: 150.459225\n",
            "Total training time: 13.12 seconds.\n",
            "-- Epoch 127\n",
            "Norm: 235.61, NNZs: 674, Bias: 71.843285, T: 7620000, Avg. loss: 145.645011\n",
            "Total training time: 13.23 seconds.\n",
            "-- Epoch 128\n",
            "Norm: 233.96, NNZs: 674, Bias: 71.943978, T: 7680000, Avg. loss: 144.603633\n",
            "Total training time: 13.33 seconds.\n",
            "-- Epoch 129\n",
            "Norm: 232.12, NNZs: 674, Bias: 72.033452, T: 7740000, Avg. loss: 142.210386\n",
            "Total training time: 13.44 seconds.\n",
            "-- Epoch 130\n",
            "Norm: 231.45, NNZs: 674, Bias: 72.145395, T: 7800000, Avg. loss: 141.037340\n",
            "Total training time: 13.55 seconds.\n",
            "-- Epoch 131\n",
            "Norm: 229.41, NNZs: 674, Bias: 72.229674, T: 7860000, Avg. loss: 140.416313\n",
            "Total training time: 13.65 seconds.\n",
            "-- Epoch 132\n",
            "Norm: 227.92, NNZs: 674, Bias: 72.323432, T: 7920000, Avg. loss: 137.347267\n",
            "Total training time: 13.75 seconds.\n",
            "-- Epoch 133\n",
            "Norm: 226.30, NNZs: 674, Bias: 72.405146, T: 7980000, Avg. loss: 139.448460\n",
            "Total training time: 13.85 seconds.\n",
            "-- Epoch 134\n",
            "Norm: 225.42, NNZs: 674, Bias: 72.519982, T: 8040000, Avg. loss: 137.423648\n",
            "Total training time: 13.96 seconds.\n",
            "-- Epoch 135\n",
            "Norm: 225.54, NNZs: 674, Bias: 72.622811, T: 8100000, Avg. loss: 138.570113\n",
            "Total training time: 14.06 seconds.\n",
            "-- Epoch 136\n",
            "Norm: 223.26, NNZs: 674, Bias: 72.719967, T: 8160000, Avg. loss: 141.263610\n",
            "Total training time: 14.16 seconds.\n",
            "-- Epoch 137\n",
            "Norm: 222.41, NNZs: 674, Bias: 72.816410, T: 8220000, Avg. loss: 132.630717\n",
            "Total training time: 14.26 seconds.\n",
            "-- Epoch 138\n",
            "Norm: 220.42, NNZs: 674, Bias: 72.878202, T: 8280000, Avg. loss: 133.009606\n",
            "Total training time: 14.37 seconds.\n",
            "-- Epoch 139\n",
            "Norm: 219.79, NNZs: 674, Bias: 72.963614, T: 8340000, Avg. loss: 134.900421\n",
            "Total training time: 14.47 seconds.\n",
            "-- Epoch 140\n",
            "Norm: 217.85, NNZs: 674, Bias: 73.030519, T: 8400000, Avg. loss: 133.833346\n",
            "Total training time: 14.58 seconds.\n",
            "-- Epoch 141\n",
            "Norm: 217.39, NNZs: 674, Bias: 73.121863, T: 8460000, Avg. loss: 130.376287\n",
            "Total training time: 14.68 seconds.\n",
            "-- Epoch 142\n",
            "Norm: 215.85, NNZs: 674, Bias: 73.194888, T: 8520000, Avg. loss: 128.296520\n",
            "Total training time: 14.78 seconds.\n",
            "-- Epoch 143\n",
            "Norm: 214.82, NNZs: 674, Bias: 73.267419, T: 8580000, Avg. loss: 131.958627\n",
            "Total training time: 14.89 seconds.\n",
            "-- Epoch 144\n",
            "Norm: 214.26, NNZs: 674, Bias: 73.363843, T: 8640000, Avg. loss: 127.924598\n",
            "Total training time: 14.99 seconds.\n",
            "-- Epoch 145\n",
            "Norm: 213.63, NNZs: 674, Bias: 73.446838, T: 8700000, Avg. loss: 127.580357\n",
            "Total training time: 15.09 seconds.\n",
            "-- Epoch 146\n",
            "Norm: 212.14, NNZs: 674, Bias: 73.535004, T: 8760000, Avg. loss: 125.339739\n",
            "Total training time: 15.20 seconds.\n",
            "-- Epoch 147\n",
            "Norm: 212.21, NNZs: 674, Bias: 73.582786, T: 8820000, Avg. loss: 126.080149\n",
            "Total training time: 15.30 seconds.\n",
            "-- Epoch 148\n",
            "Norm: 210.61, NNZs: 674, Bias: 73.665263, T: 8880000, Avg. loss: 123.795326\n",
            "Total training time: 15.40 seconds.\n",
            "-- Epoch 149\n",
            "Norm: 209.91, NNZs: 674, Bias: 73.735940, T: 8940000, Avg. loss: 121.084502\n",
            "Total training time: 15.51 seconds.\n",
            "-- Epoch 150\n",
            "Norm: 208.59, NNZs: 674, Bias: 73.827340, T: 9000000, Avg. loss: 122.621697\n",
            "Total training time: 15.61 seconds.\n",
            "-- Epoch 151\n",
            "Norm: 208.19, NNZs: 674, Bias: 73.919277, T: 9060000, Avg. loss: 125.307075\n",
            "Total training time: 15.71 seconds.\n",
            "-- Epoch 152\n",
            "Norm: 207.73, NNZs: 674, Bias: 73.998474, T: 9120000, Avg. loss: 120.947805\n",
            "Total training time: 15.82 seconds.\n",
            "-- Epoch 153\n",
            "Norm: 206.02, NNZs: 674, Bias: 74.082661, T: 9180000, Avg. loss: 121.201935\n",
            "Total training time: 15.92 seconds.\n",
            "-- Epoch 154\n",
            "Norm: 205.43, NNZs: 674, Bias: 74.167279, T: 9240000, Avg. loss: 122.016105\n",
            "Total training time: 16.02 seconds.\n",
            "-- Epoch 155\n",
            "Norm: 204.57, NNZs: 674, Bias: 74.244925, T: 9300000, Avg. loss: 118.739327\n",
            "Total training time: 16.13 seconds.\n",
            "-- Epoch 156\n",
            "Norm: 203.94, NNZs: 674, Bias: 74.304994, T: 9360000, Avg. loss: 117.821251\n",
            "Total training time: 16.24 seconds.\n",
            "-- Epoch 157\n",
            "Norm: 202.98, NNZs: 674, Bias: 74.378473, T: 9420000, Avg. loss: 117.788004\n",
            "Total training time: 16.34 seconds.\n",
            "-- Epoch 158\n",
            "Norm: 202.08, NNZs: 674, Bias: 74.456778, T: 9480000, Avg. loss: 115.077652\n",
            "Total training time: 16.44 seconds.\n",
            "-- Epoch 159\n",
            "Norm: 200.89, NNZs: 674, Bias: 74.537723, T: 9540000, Avg. loss: 115.640391\n",
            "Total training time: 16.54 seconds.\n",
            "-- Epoch 160\n",
            "Norm: 199.41, NNZs: 674, Bias: 74.611904, T: 9600000, Avg. loss: 114.666449\n",
            "Total training time: 16.65 seconds.\n",
            "-- Epoch 161\n",
            "Norm: 199.15, NNZs: 674, Bias: 74.683505, T: 9660000, Avg. loss: 115.263918\n",
            "Total training time: 16.75 seconds.\n",
            "-- Epoch 162\n",
            "Norm: 198.37, NNZs: 674, Bias: 74.773284, T: 9720000, Avg. loss: 113.862860\n",
            "Total training time: 16.85 seconds.\n",
            "-- Epoch 163\n",
            "Norm: 197.43, NNZs: 674, Bias: 74.849172, T: 9780000, Avg. loss: 110.468448\n",
            "Total training time: 16.95 seconds.\n",
            "-- Epoch 164\n",
            "Norm: 197.33, NNZs: 674, Bias: 74.927664, T: 9840000, Avg. loss: 113.945809\n",
            "Total training time: 17.06 seconds.\n",
            "-- Epoch 165\n",
            "Norm: 196.45, NNZs: 674, Bias: 75.012766, T: 9900000, Avg. loss: 114.187896\n",
            "Total training time: 17.16 seconds.\n",
            "-- Epoch 166\n",
            "Norm: 195.10, NNZs: 674, Bias: 75.072146, T: 9960000, Avg. loss: 111.557810\n",
            "Total training time: 17.27 seconds.\n",
            "-- Epoch 167\n",
            "Norm: 194.20, NNZs: 674, Bias: 75.134208, T: 10020000, Avg. loss: 110.278487\n",
            "Total training time: 17.37 seconds.\n",
            "-- Epoch 168\n",
            "Norm: 193.62, NNZs: 674, Bias: 75.205842, T: 10080000, Avg. loss: 110.091520\n",
            "Total training time: 17.48 seconds.\n",
            "-- Epoch 169\n",
            "Norm: 192.98, NNZs: 674, Bias: 75.276095, T: 10140000, Avg. loss: 108.293034\n",
            "Total training time: 17.58 seconds.\n",
            "-- Epoch 170\n",
            "Norm: 192.57, NNZs: 674, Bias: 75.359671, T: 10200000, Avg. loss: 107.587755\n",
            "Total training time: 17.69 seconds.\n",
            "-- Epoch 171\n",
            "Norm: 191.99, NNZs: 674, Bias: 75.420271, T: 10260000, Avg. loss: 107.925838\n",
            "Total training time: 17.79 seconds.\n",
            "-- Epoch 172\n",
            "Norm: 191.01, NNZs: 674, Bias: 75.468848, T: 10320000, Avg. loss: 105.388800\n",
            "Total training time: 17.89 seconds.\n",
            "-- Epoch 173\n",
            "Norm: 190.03, NNZs: 674, Bias: 75.555807, T: 10380000, Avg. loss: 106.272312\n",
            "Total training time: 18.00 seconds.\n",
            "-- Epoch 174\n",
            "Norm: 189.32, NNZs: 674, Bias: 75.626866, T: 10440000, Avg. loss: 106.088715\n",
            "Total training time: 18.10 seconds.\n",
            "-- Epoch 175\n",
            "Norm: 188.78, NNZs: 674, Bias: 75.691809, T: 10500000, Avg. loss: 105.511782\n",
            "Total training time: 18.20 seconds.\n",
            "-- Epoch 176\n",
            "Norm: 188.15, NNZs: 674, Bias: 75.764934, T: 10560000, Avg. loss: 104.828172\n",
            "Total training time: 18.31 seconds.\n",
            "-- Epoch 177\n",
            "Norm: 187.79, NNZs: 674, Bias: 75.836689, T: 10620000, Avg. loss: 103.175191\n",
            "Total training time: 18.41 seconds.\n",
            "-- Epoch 178\n",
            "Norm: 187.10, NNZs: 674, Bias: 75.911782, T: 10680000, Avg. loss: 104.313347\n",
            "Total training time: 18.51 seconds.\n",
            "-- Epoch 179\n",
            "Norm: 186.14, NNZs: 674, Bias: 75.978095, T: 10740000, Avg. loss: 102.732051\n",
            "Total training time: 18.62 seconds.\n",
            "-- Epoch 180\n",
            "Norm: 185.48, NNZs: 674, Bias: 76.041208, T: 10800000, Avg. loss: 101.889026\n",
            "Total training time: 18.72 seconds.\n",
            "-- Epoch 181\n",
            "Norm: 184.57, NNZs: 674, Bias: 76.110452, T: 10860000, Avg. loss: 100.766248\n",
            "Total training time: 18.82 seconds.\n",
            "-- Epoch 182\n",
            "Norm: 183.53, NNZs: 674, Bias: 76.168304, T: 10920000, Avg. loss: 100.875087\n",
            "Total training time: 18.92 seconds.\n",
            "-- Epoch 183\n",
            "Norm: 182.82, NNZs: 674, Bias: 76.228588, T: 10980000, Avg. loss: 101.240166\n",
            "Total training time: 19.02 seconds.\n",
            "-- Epoch 184\n",
            "Norm: 182.27, NNZs: 674, Bias: 76.290365, T: 11040000, Avg. loss: 99.483486\n",
            "Total training time: 19.13 seconds.\n",
            "-- Epoch 185\n",
            "Norm: 182.21, NNZs: 674, Bias: 76.356276, T: 11100000, Avg. loss: 98.133588\n",
            "Total training time: 19.23 seconds.\n",
            "-- Epoch 186\n",
            "Norm: 181.85, NNZs: 674, Bias: 76.437147, T: 11160000, Avg. loss: 99.694713\n",
            "Total training time: 19.34 seconds.\n",
            "-- Epoch 187\n",
            "Norm: 180.97, NNZs: 674, Bias: 76.506827, T: 11220000, Avg. loss: 99.317092\n",
            "Total training time: 19.44 seconds.\n",
            "-- Epoch 188\n",
            "Norm: 180.14, NNZs: 674, Bias: 76.571725, T: 11280000, Avg. loss: 99.029782\n",
            "Total training time: 19.54 seconds.\n",
            "-- Epoch 189\n",
            "Norm: 179.23, NNZs: 674, Bias: 76.642448, T: 11340000, Avg. loss: 96.767918\n",
            "Total training time: 19.65 seconds.\n",
            "-- Epoch 190\n",
            "Norm: 178.31, NNZs: 674, Bias: 76.706659, T: 11400000, Avg. loss: 98.021854\n",
            "Total training time: 19.75 seconds.\n",
            "-- Epoch 191\n",
            "Norm: 177.69, NNZs: 674, Bias: 76.771382, T: 11460000, Avg. loss: 95.812349\n",
            "Total training time: 19.85 seconds.\n",
            "-- Epoch 192\n",
            "Norm: 176.94, NNZs: 674, Bias: 76.847092, T: 11520000, Avg. loss: 95.333186\n",
            "Total training time: 19.96 seconds.\n",
            "-- Epoch 193\n",
            "Norm: 176.38, NNZs: 674, Bias: 76.913746, T: 11580000, Avg. loss: 96.189957\n",
            "Total training time: 20.06 seconds.\n",
            "-- Epoch 194\n",
            "Norm: 176.25, NNZs: 674, Bias: 76.983496, T: 11640000, Avg. loss: 93.026106\n",
            "Total training time: 20.16 seconds.\n",
            "-- Epoch 195\n",
            "Norm: 175.60, NNZs: 674, Bias: 77.044325, T: 11700000, Avg. loss: 96.549103\n",
            "Total training time: 20.27 seconds.\n",
            "-- Epoch 196\n",
            "Norm: 174.40, NNZs: 674, Bias: 77.098871, T: 11760000, Avg. loss: 92.570759\n",
            "Total training time: 20.37 seconds.\n",
            "-- Epoch 197\n",
            "Norm: 173.69, NNZs: 674, Bias: 77.140414, T: 11820000, Avg. loss: 94.770677\n",
            "Total training time: 20.48 seconds.\n",
            "-- Epoch 198\n",
            "Norm: 173.33, NNZs: 674, Bias: 77.203707, T: 11880000, Avg. loss: 92.741716\n",
            "Total training time: 20.58 seconds.\n",
            "-- Epoch 199\n",
            "Norm: 172.81, NNZs: 674, Bias: 77.267509, T: 11940000, Avg. loss: 93.860160\n",
            "Total training time: 20.69 seconds.\n",
            "-- Epoch 200\n",
            "Norm: 172.02, NNZs: 674, Bias: 77.321811, T: 12000000, Avg. loss: 91.740944\n",
            "Total training time: 20.79 seconds.\n",
            "-- Epoch 201\n",
            "Norm: 171.70, NNZs: 674, Bias: 77.379194, T: 12060000, Avg. loss: 92.022771\n",
            "Total training time: 20.89 seconds.\n",
            "-- Epoch 202\n",
            "Norm: 171.04, NNZs: 674, Bias: 77.419707, T: 12120000, Avg. loss: 89.081766\n",
            "Total training time: 21.00 seconds.\n",
            "-- Epoch 203\n",
            "Norm: 170.55, NNZs: 674, Bias: 77.479787, T: 12180000, Avg. loss: 89.989131\n",
            "Total training time: 21.10 seconds.\n",
            "-- Epoch 204\n",
            "Norm: 169.56, NNZs: 674, Bias: 77.530561, T: 12240000, Avg. loss: 89.522240\n",
            "Total training time: 21.20 seconds.\n",
            "-- Epoch 205\n",
            "Norm: 168.84, NNZs: 674, Bias: 77.584349, T: 12300000, Avg. loss: 90.916005\n",
            "Total training time: 21.31 seconds.\n",
            "-- Epoch 206\n",
            "Norm: 168.38, NNZs: 674, Bias: 77.643551, T: 12360000, Avg. loss: 90.077114\n",
            "Total training time: 21.41 seconds.\n",
            "-- Epoch 207\n",
            "Norm: 167.76, NNZs: 674, Bias: 77.692779, T: 12420000, Avg. loss: 87.562197\n",
            "Total training time: 21.51 seconds.\n",
            "-- Epoch 208\n",
            "Norm: 167.15, NNZs: 674, Bias: 77.748980, T: 12480000, Avg. loss: 88.042941\n",
            "Total training time: 21.62 seconds.\n",
            "-- Epoch 209\n",
            "Norm: 166.69, NNZs: 674, Bias: 77.786573, T: 12540000, Avg. loss: 87.227596\n",
            "Total training time: 21.72 seconds.\n",
            "-- Epoch 210\n",
            "Norm: 166.41, NNZs: 674, Bias: 77.845422, T: 12600000, Avg. loss: 86.923006\n",
            "Total training time: 21.82 seconds.\n",
            "-- Epoch 211\n",
            "Norm: 165.94, NNZs: 674, Bias: 77.905556, T: 12660000, Avg. loss: 86.991639\n",
            "Total training time: 21.93 seconds.\n",
            "-- Epoch 212\n",
            "Norm: 165.18, NNZs: 674, Bias: 77.961503, T: 12720000, Avg. loss: 86.841441\n",
            "Total training time: 22.03 seconds.\n",
            "-- Epoch 213\n",
            "Norm: 164.73, NNZs: 674, Bias: 78.013270, T: 12780000, Avg. loss: 85.596716\n",
            "Total training time: 22.13 seconds.\n",
            "-- Epoch 214\n",
            "Norm: 163.99, NNZs: 674, Bias: 78.064776, T: 12840000, Avg. loss: 84.692564\n",
            "Total training time: 22.24 seconds.\n",
            "-- Epoch 215\n",
            "Norm: 163.18, NNZs: 674, Bias: 78.119166, T: 12900000, Avg. loss: 85.901784\n",
            "Total training time: 22.34 seconds.\n",
            "-- Epoch 216\n",
            "Norm: 162.89, NNZs: 674, Bias: 78.167113, T: 12960000, Avg. loss: 85.575487\n",
            "Total training time: 22.45 seconds.\n",
            "-- Epoch 217\n",
            "Norm: 162.25, NNZs: 674, Bias: 78.227163, T: 13020000, Avg. loss: 85.189470\n",
            "Total training time: 22.55 seconds.\n",
            "-- Epoch 218\n",
            "Norm: 161.51, NNZs: 674, Bias: 78.283108, T: 13080000, Avg. loss: 87.289568\n",
            "Total training time: 22.65 seconds.\n",
            "-- Epoch 219\n",
            "Norm: 161.05, NNZs: 674, Bias: 78.338026, T: 13140000, Avg. loss: 83.277718\n",
            "Total training time: 22.76 seconds.\n",
            "-- Epoch 220\n",
            "Norm: 160.61, NNZs: 674, Bias: 78.389647, T: 13200000, Avg. loss: 83.755371\n",
            "Total training time: 22.86 seconds.\n",
            "-- Epoch 221\n",
            "Norm: 160.08, NNZs: 674, Bias: 78.438762, T: 13260000, Avg. loss: 83.780850\n",
            "Total training time: 22.96 seconds.\n",
            "-- Epoch 222\n",
            "Norm: 159.48, NNZs: 674, Bias: 78.490682, T: 13320000, Avg. loss: 81.474532\n",
            "Total training time: 23.06 seconds.\n",
            "-- Epoch 223\n",
            "Norm: 158.91, NNZs: 674, Bias: 78.535607, T: 13380000, Avg. loss: 82.275435\n",
            "Total training time: 23.17 seconds.\n",
            "-- Epoch 224\n",
            "Norm: 158.41, NNZs: 674, Bias: 78.588566, T: 13440000, Avg. loss: 82.728876\n",
            "Total training time: 23.27 seconds.\n",
            "-- Epoch 225\n",
            "Norm: 158.13, NNZs: 674, Bias: 78.654631, T: 13500000, Avg. loss: 80.756246\n",
            "Total training time: 23.37 seconds.\n",
            "-- Epoch 226\n",
            "Norm: 157.36, NNZs: 674, Bias: 78.698227, T: 13560000, Avg. loss: 81.068833\n",
            "Total training time: 23.48 seconds.\n",
            "-- Epoch 227\n",
            "Norm: 157.16, NNZs: 674, Bias: 78.756362, T: 13620000, Avg. loss: 81.708832\n",
            "Total training time: 23.58 seconds.\n",
            "-- Epoch 228\n",
            "Norm: 156.57, NNZs: 674, Bias: 78.808376, T: 13680000, Avg. loss: 81.766142\n",
            "Total training time: 23.69 seconds.\n",
            "-- Epoch 229\n",
            "Norm: 155.66, NNZs: 674, Bias: 78.866718, T: 13740000, Avg. loss: 80.704641\n",
            "Total training time: 23.79 seconds.\n",
            "-- Epoch 230\n",
            "Norm: 155.03, NNZs: 674, Bias: 78.929916, T: 13800000, Avg. loss: 80.626325\n",
            "Total training time: 23.90 seconds.\n",
            "-- Epoch 231\n",
            "Norm: 155.02, NNZs: 674, Bias: 78.979075, T: 13860000, Avg. loss: 77.962691\n",
            "Total training time: 24.00 seconds.\n",
            "-- Epoch 232\n",
            "Norm: 154.50, NNZs: 674, Bias: 79.028015, T: 13920000, Avg. loss: 78.202565\n",
            "Total training time: 24.10 seconds.\n",
            "-- Epoch 233\n",
            "Norm: 153.85, NNZs: 674, Bias: 79.083928, T: 13980000, Avg. loss: 78.628047\n",
            "Total training time: 24.21 seconds.\n",
            "-- Epoch 234\n",
            "Norm: 153.91, NNZs: 674, Bias: 79.142440, T: 14040000, Avg. loss: 76.447151\n",
            "Total training time: 24.31 seconds.\n",
            "-- Epoch 235\n",
            "Norm: 153.35, NNZs: 674, Bias: 79.202858, T: 14100000, Avg. loss: 80.019113\n",
            "Total training time: 24.42 seconds.\n",
            "-- Epoch 236\n",
            "Norm: 152.93, NNZs: 674, Bias: 79.251700, T: 14160000, Avg. loss: 78.022196\n",
            "Total training time: 24.52 seconds.\n",
            "-- Epoch 237\n",
            "Norm: 152.72, NNZs: 674, Bias: 79.301728, T: 14220000, Avg. loss: 78.698093\n",
            "Total training time: 24.62 seconds.\n",
            "-- Epoch 238\n",
            "Norm: 152.05, NNZs: 674, Bias: 79.342428, T: 14280000, Avg. loss: 77.607774\n",
            "Total training time: 24.73 seconds.\n",
            "-- Epoch 239\n",
            "Norm: 151.52, NNZs: 674, Bias: 79.387858, T: 14340000, Avg. loss: 77.870948\n",
            "Total training time: 24.83 seconds.\n",
            "Convergence after 239 epochs took 24.83 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
              "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
              "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
              "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
              "              power_t=0.5, random_state=42, shuffle=True, tol=0.001,\n",
              "              validation_fraction=0.1, verbose=1, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSNIMyuUcj8E"
      },
      "source": [
        "# 이 모델을 사용해서 숫자 5의 이미지를 감지해본다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5WoEs8Xcm5o",
        "outputId": "3fc58898-78ba-4694-b3b3-3cef4830458c"
      },
      "source": [
        "sgd_clf.predict([some_digit])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yonUD6egpmEi"
      },
      "source": [
        "#  여기서 잠깐, StratifiedShuffledSplit에 대한 설명.\r\n",
        "\r\n",
        "원래의 데이터셋에서 각 클래스마다의 비율을 유지해 준채 훈련과 테스트 데이터셋을 분리해준다. 섞는 횟수는 자유임\r\n",
        "\r\n",
        "근데 이게 StratifiedKfold랑 무슨 차이인지 아직은 잘 모르겠다.\r\n",
        "\r\n",
        "그냥 섞느냐 안섞느냐의 차이인것 같기도 하다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uqqFmNZjTDn",
        "outputId": "a2c1ce41-c655-44e3-cbe3-acf97ef12a6c"
      },
      "source": [
        "import numpy as np\r\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\r\n",
        "xx= np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\r\n",
        "yy = np.array([0, 0, 0, 1, 1, 1])\r\n",
        "sss = StratifiedShuffleSplit(n_splits=6, test_size=0.5, random_state=0)\r\n",
        "sss.get_n_splits(xx, yy) #몇번 섞었는지 알려준다.\r\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJ7sPyAZjXBQ",
        "outputId": "84c87de7-0eee-4ff1-a979-e14d12c1b488"
      },
      "source": [
        "for train_index, test_index in sss.split(xx, yy):\r\n",
        "  print(\"TRAIN:\", train_index, \"TEST:\", test_index)\r\n",
        "  xx_train, xx_test = xx[train_index], xx[test_index]\r\n",
        "  yy_train, yy_test = yy[train_index], yy[test_index]\r\n",
        "print(xx_train) # 마지막에 섞인 폴드를 가지고 반환\r\n",
        "print(xx_test)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [5 2 3] TEST: [4 1 0]\n",
            "TRAIN: [5 1 4] TEST: [0 2 3]\n",
            "TRAIN: [5 0 2] TEST: [4 3 1]\n",
            "TRAIN: [4 1 0] TEST: [2 3 5]\n",
            "TRAIN: [0 5 1] TEST: [3 4 2]\n",
            "TRAIN: [2 3 4] TEST: [0 5 1]\n",
            "[[1 2]\n",
            " [3 4]\n",
            " [1 2]]\n",
            "[[1 2]\n",
            " [3 4]\n",
            " [3 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g5lf14tmbfD"
      },
      "source": [
        "# KFold 와 무슨 차이일까?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjbkWcv2yf7j",
        "outputId": "8e82fbae-5eb5-454b-a897-221cd46f4c9f"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "sk=StratifiedKFold(n_splits=3, random_state=0, shuffle=True) #n_splits는 fold의 수를 의미한다.\r\n",
        "\r\n",
        "for train_index, test_index in sk.split(iing_x, iing_y):\r\n",
        "  print(\"TRAIN:\", train_index, \"TEST:\", test_index)\r\n",
        "  xxx_train, xxx_test = iing['gender'].loc[train_index], iing['gender'].loc[test_index]\r\n",
        "  yyy_train, yyx_test = iing['education'].loc[train_index], iing['education'].loc[test_index]\r\n"
      ],
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [    0     2     3 ... 45527 45528 45529] TEST: [    1     4     7 ... 45524 45530 45531]\n",
            "TRAIN: [    1     4     6 ... 45527 45530 45531] TEST: [    0     2     3 ... 45526 45528 45529]\n",
            "TRAIN: [    0     1     2 ... 45529 45530 45531] TEST: [    6    10    18 ... 45523 45525 45527]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tK1Roirytow",
        "outputId": "b75d93ee-4f87-4148-8e60-87b4851aedc3"
      },
      "source": [
        " print(xxx_train.value_counts()/len(xxx_train))\r\n",
        "print(yyy_train.value_counts()/len(yyy_train))"
      ],
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Male      0.535623\n",
            "Female    0.464377\n",
            "Name: gender, dtype: float64\n",
            "2    0.369762\n",
            "3    0.334402\n",
            "4    0.172494\n",
            "1    0.111746\n",
            "0    0.011596\n",
            "Name: education, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zqMVJ9Bz-j7",
        "outputId": "2a421133-fa57-4116-85de-b3ad60e70650"
      },
      "source": [
        "print(xxx_test.value_counts()/len(xxx_test))\r\n",
        "print(yyx_test.value_counts()/len(yyx_test))"
      ],
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Male      0.528112\n",
            "Female    0.471888\n",
            "Name: gender, dtype: float64\n",
            "2    0.369718\n",
            "3    0.334446\n",
            "4    0.172494\n",
            "1    0.111746\n",
            "0    0.011596\n",
            "Name: education, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUYLDg5BzLln",
        "outputId": "acb3a48f-da89-476d-b36c-27d0735456b2"
      },
      "source": [
        " print(iing['gender'].value_counts()/len(iing))\r\n",
        "print(iing['education'].value_counts()/len(iing))"
      ],
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Male      0.531868\n",
            "Female    0.468132\n",
            "Name: gender, dtype: float64\n",
            "2    0.369740\n",
            "3    0.334424\n",
            "4    0.172494\n",
            "1    0.111746\n",
            "0    0.011596\n",
            "Name: education, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ciub3IxG2CjD"
      },
      "source": [
        "# n_splits=2로 하면 테스트와 트레인의 비율이 똑같다. 3으로하면 트레인데이터비율이 더 많아진다. 그러니까 숫자가 올라갈수록 테스트 사이즈는 줄어드는 것이다.\r\n",
        "\r\n",
        "shuffle=True 로 설정해도, 인덱스는 순차적으로 쌓여간다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajHGWejB2N_q",
        "outputId": "10bbf73a-6326-4655-b222-53c18a0af7af"
      },
      "source": [
        "xxx_train"
      ],
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        Female\n",
              "1        Female\n",
              "2          Male\n",
              "3        Female\n",
              "4          Male\n",
              "          ...  \n",
              "45526    Female\n",
              "45528      Male\n",
              "45529    Female\n",
              "45530      Male\n",
              "45531      Male\n",
              "Name: gender, Length: 30355, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 273
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sbCBs1u489u",
        "outputId": "6e96d2db-03a9-475e-8238-9fef2dbf22b2"
      },
      "source": [
        "xxx_test"
      ],
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6          Male\n",
              "10       Female\n",
              "18       Female\n",
              "24       Female\n",
              "26       Female\n",
              "          ...  \n",
              "45520    Female\n",
              "45521      Male\n",
              "45523    Female\n",
              "45525      Male\n",
              "45527    Female\n",
              "Name: gender, Length: 15177, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 274
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI6PeIyhzzYy",
        "outputId": "930f3e63-214b-489b-b65e-b684d93c32b4"
      },
      "source": [
        "print(len(iing))\r\n",
        "print(len(xxx_train))\r\n",
        "print(len(xxx_test))"
      ],
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45532\n",
            "22766\n",
            "22766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKqyiVh_6Pcz",
        "outputId": "7897b4bd-5aaa-4e70-ca5f-decaec77060e"
      },
      "source": [
        ""
      ],
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34149.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 250
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smMYWmwk1ZM3",
        "outputId": "1bd68244-491f-41d8-ac9b-fa38be494aba"
      },
      "source": [
        "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\r\n",
        "for train_index, test_index in sss.split(iing_x, iing_y):\r\n",
        "  print(\"TRAIN:\", train_index, \"TEST:\", test_index)\r\n",
        "  xx_train, xx_test = iing['gender'].loc[train_index], iing['gender'].loc[test_index]\r\n",
        "  yy_train, yy_test = iing['education'].loc[train_index], iing['education'].loc[test_index]\r\n"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [11823  5667 22716 ... 28114 37159 16283] TEST: [ 3086 34775 22286 ... 44304 44319 32695]\n",
            "TRAIN: [17988  9181 30261 ... 21762 15791 16764] TEST: [17698 21679 35749 ... 43237 37294 43521]\n",
            "TRAIN: [39522 35807 27692 ... 39666 20393 37420] TEST: [32529   175 15206 ... 34484 20452 25140]\n",
            "TRAIN: [32546 37553 38658 ... 29222 10385 20976] TEST: [42613 30086  5728 ... 27639 20502 42703]\n",
            "TRAIN: [20870 18470 23534 ... 33640 40534 23952] TEST: [24474 31293  3261 ... 27537 22272 26769]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSDOS0qw3v45",
        "outputId": "26ef918f-ad2a-484d-95c4-e56da1fd9535"
      },
      "source": [
        "print(xx_train.value_counts()/len(xx_train))\r\n",
        "print(yy_train.value_counts()/len(yy_train))"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Male      0.530434\n",
            "Female    0.469566\n",
            "Name: gender, dtype: float64\n",
            "2    0.369729\n",
            "3    0.334431\n",
            "4    0.172503\n",
            "1    0.111728\n",
            "0    0.011609\n",
            "Name: education, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2MYclJb3-My",
        "outputId": "0eb84f5e-93e1-46de-d50d-0a9c99bc6945"
      },
      "source": [
        "print(xx_test.value_counts()/len(xx_test))\r\n",
        "print(yy_test.value_counts()/len(yy_test))"
      ],
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Male      0.535212\n",
            "Female    0.464788\n",
            "Name: gender, dtype: float64\n",
            "2    0.369766\n",
            "3    0.334407\n",
            "4    0.172474\n",
            "1    0.111786\n",
            "0    0.011567\n",
            "Name: education, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2qlYIHI4Cbi",
        "outputId": "dfe3965d-b2bf-429f-f7f0-65c0596aa643"
      },
      "source": [
        "print(iing['gender'].value_counts()/len(iing))\r\n",
        "print(iing['education'].value_counts()/len(iing))"
      ],
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Male      0.531868\n",
            "Female    0.468132\n",
            "Name: gender, dtype: float64\n",
            "2    0.369740\n",
            "3    0.334424\n",
            "4    0.172494\n",
            "1    0.111746\n",
            "0    0.011596\n",
            "Name: education, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBREQtwK4GV5",
        "outputId": "5e62bb60-1c7b-4536-80bc-fce50671725e"
      },
      "source": [
        "print(len(iing))\r\n",
        "print(len(xx_train))\r\n",
        "print(len(xx_test))"
      ],
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45532\n",
            "31872\n",
            "13660\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhX3sb6M4KFY",
        "outputId": "0809a840-0319-402c-8102-be2d248de9e1"
      },
      "source": [
        "xx_train"
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20870    Female\n",
              "18470    Female\n",
              "23534    Female\n",
              "21359    Female\n",
              "2089       Male\n",
              "          ...  \n",
              "16220    Female\n",
              "11155      Male\n",
              "33640    Female\n",
              "40534    Female\n",
              "23952      Male\n",
              "Name: gender, Length: 31872, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 237
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP1tH0C74vy5"
      },
      "source": [
        "# 실험결과 정리\r\n",
        "\r\n",
        "StratifiedShuffleSplit 특징 :\r\n",
        "\r\n",
        "1. 데이터를 무작위로 섞는다.\r\n",
        "2. 만약 어떤 변수가 타겟과 중요하게 연관되어 있다면(상관계수가 높다거나..) 그 변수를 기준으로 트레인 데이터와 테스트 데이터가\r\n",
        "  원본데이터와 비슷한 비율(분포)을 유지하는것이 중요하다. 그리고 이것은 그 기능을 잘 수행해준다.\r\n",
        "3. n_splits의 용도는 몇 번 섞을지 정한다. 많이 섞느냐 적게 섞느냐 중요한지 아직은 모르겠다. 순서가 중요한 데이터라면(시계열처럼) 안 섞는게 좋을것 같다.\r\n",
        "\r\n",
        "\r\n",
        "StratifiedKFold 특징 :\r\n",
        "\r\n",
        "1. 데이터를 섞지 않는다.\r\n",
        "2. 이것도 똑같이 원본데이터셋의 비율을 유지해주는데 중요한것은 순서를 뒤짚지는 않는다.\r\n",
        "3. n_splits의 용도는 StratifiedShuffleSplit과는 다르게, 몇 번 섞을지 정하는게 아니라, 훈련 데이터와 테스트 데이터의 비율을 조정해준다.\r\n",
        "  예를 들어서 n_splits=2 라면, 1:1의 비율로 자르는 것이다.\r\n",
        "  만약에 n_splits=3 이라면, 1:2의 비율로 자르는 것이다.(훈련데이터가 2/3, 테스트 데이터가 1/3)\r\n",
        "  결론적으로는 이 파라미터가 늘어날수록 테스트데이터는 줄어든다는 것이다.\r\n",
        "4.shuffle 파라미터는 데이터순서를 뒤섞는게아니라, 훈련 데이터와 테스트 데이터를 교차로 번갈아가며 데이터를 분할해 주는듯 하다.\r\n",
        "  이것을 False로 지정하면, 훈련 데이터를 인덱스 순으로 먼저 비율을 맞추어주고, 나머지 테스트 데이터 비율을 맞춘다.\r\n",
        "  그래서 트레인 데이터는 인덱스(순서)가 0부터 증가한다. 혹은, 순서대로 차근차근 증가한다.\r\n",
        "  그리고 비율이 맞춰진 이후에 남은 인덱스로 테스트데이터를 채우기 시작한다.\r\n",
        "\r\n",
        "  이것을 True로 지정하면, 훈련데이터와 테스트데이터의 비율을 동시에 맞추기 시작한다.(그러면 인덱스가 교차로 부여된다.)\r\n",
        "  그렇다고 훈련 데이터 및 테스트 데이터내의 순서가 섞이는 것은 아니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfzNop7J9jjU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}